\documentclass[twocolumn]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=0.8in]{geometry} %Margins
\usepackage{multicol} %Columns: multiples
\usepackage[hidelinks]{hyperref} %Hyperlinks
\usepackage{authblk} %Authors: affiliation
\renewcommand\Affilfont{\itshape\small} %Authors: affiliation font
\usepackage{tabularx} % Tabular: automatic line-break
\usepackage[justification=raggedright]{caption} %Floats: caption as with tables
\usepackage{ltablex} %Floats: long
\renewcommand{\arraystretch}{1.5} %Tables: more space between rows
\usepackage{graphicx} %Figures: import
\graphicspath{{../pics/}} %Figures path
\usepackage{subcaption} %Figures: multiple with one caption
\usepackage{amsmath} %Case definition
\usepackage[toc,page]{appendix} %Appendix
\newenvironment{conditions} %Conditions
{\par\vspace{\abovedisplayskip}\noindent\begin{tabular}{>{$}l<{$} @{${}={}$} l}} %Conditions
{\end{tabular}\par\vspace{\belowdisplayskip}} %Conditions
\usepackage{natbib} %Citations: APA
\usepackage{amsmath} %Use \[ ... \]
\usepackage{xcolor} %colors
\title{Food web aggregation: effects on key positions}
\author[1]{Emanuele Giacomuzzo}
\author[1]{Ferenc Jordàn}
\affil[1]{Stazione Zoologica Anton Dohrn, Napoli, 80122, Italy}
%\affil[2]{Central European University, Budapest, 1051, Hungary}
\date{}
\begin{document}
\maketitle
\section*{Introduction}

	Trophic data management is something that ecologists always must deal with when working with food webs. Trophic interactions can be described among individuals, life stages, species, higher taxa, functional groups, and several other, appropriately defined nodes of food webs. Some kind of aggregation is unavoidable, even the most highly resoluted food webs contain big aggregates (e.g., “bacteria'', see \citet{Martinez1991}). At the same time, even the least resoluted food webs may contain species (e.g., “hake”, see \citet{Yodzis1998}). Data aggregation can happen also at later stages, during data analysis, especially in large networks, where the study of hundreds of nodes would be unfeasible \citep{Yodzis1999}.

	Data aggregation methods are problem-dependent. Not considering this can bias the way by which we interpret the results of food web models \citep{Paine1988, Hall1993}.For instance, various levels of aggregation at different trophic levels might bias our interpretation if we are trying to characterise the structure of a network \citep{Yodzis1999}.Both low- and high-resolution networks can be useful or useless, the key challenge is to properly match the problem, the data management, and the model construction. Even if this seems like a ubiquitous problem in food web ecology, standards for whether and how to aggregate data in a meaningful way does not exist yet.

	The process of data aggregation assumes that there are nodes in the network that are similar enough that we can consider them functionally equivalent.
	For example, two fishes from the same genus might be aggregated into a node of the genus (e.g., \emph{Poecilia sphenops} and \emph{Poecilia reticulata} could be aggregated into \emph{Poecilia}).

	Similarity can be understood mathematically (equivalent network positions) and biologically (similar trophic habits).	\citet{Yodzis1999} and \citet{Luczkovich2003} tried to answer this question by borrowing two definitions from social networks. \citet{Yodzis1999} borrowed the concept of structural equivalence – where two nodes are similar when sharing a high number of neighbours – and called the aggregation of structurally equivalent species” trophospecies”.
	\citet{Luczkovich2003} borrowed the concept of regular equivalence – where two nodes are similar when sharing a high number of similar but not necessarily the same neighbours. Nodes belonging to the same equivalence class share ecological roles.

	Groups of nodes that have different neighbours but form dense subgraphs are called modules. Species in food web modules can play different roles (e.g. predator and prey) but they maintain well-defined multi-species processes (e.g. connecting benthic and pelagic organisms). Aggregating the modules of a food web has been suggested already by \citet{Allesina2009a}. The two most reliable ways of finding modules in food webs are through the group model and modularity maximisation. The group model was firstly developed by \citet{Allesina2009a} and then extended by \citet{Sander2015}.	Modularity maximisation was firstly applied to food webs by \citet{Guimera2010} following three definitions of modularity.	The first one, which we will refer to as density-based modularity, is the degree by which nodes inside modules interact more among themselves than with nodes of other modules.	The second one, which we will refer to as prey-based modularity, is the degree by which nodes inside modules tend to interact with the same predators.	The third one, which we will refer to as predator-based modularity, is the degree by which nodes inside modules tend to interact with the same preys.

	The positional importance of species differs in both highly-aggregated and highly-resoluted networks. Central positions may be a proxy for functional importance and the community-wide distribution of either centrality values \citep{Bauer2010} or hypothetical importance values \citep{Mills1993} provide macroscopic descriptors of ecosystems.

	In this paper, we investigate how these different aggregation methods maintain the relative importance of species, as a proxy of network structure.
	To compute the importance of species we used 25 of the most used centrality indices used in keystone species research.
	Our investigation was carried out on 86 Ecopath with Ecosim food web models. By having been constructed with the same methodology (see \citet{Okey2004}), they were easy to compare. These models were freely available on the EcoBase database \citep{Colleter2013}. The way we selected the food webs to be included in our analysis was the number of nodes: we selected only the food webs with at least 14 nodes.  See a table of these food webs in Appendix \ref{appendix:food_webs}. %CHANGE

						%\begin{figure}[htbp]%{\textwidth}
						%	\centering
						%	\includegraphics[width=1.0\linewidth]{reg_struct_equivalence}
						%	\caption{Two types of similarity indices highly used in social networks, which have been applied also to food webs. As you can see, two nodes are regularly equivalent if they are connected to similar nodes (b) and structurally equivalent if they are connected to the same exact nodes (a). Two structurally equivalent nodes are also regularly equivalent, but not the other way around. For example, two nurses are regularly equivalent because they have the same connections to other personnel in the hospital such as doctors, other nurses, receptionists, patients and so on. If the personnel they are in contact with not only is similar but it's the same exact persons, then they are also structurally equivalent.}
						%	\label{fig:equivalences}
						%\end{figure} %CHANGE

\section*{Methods: clustering techniques}

	To cluster similar nodes, we used the following clustering techinques.

	\subsection*{Hierarchical clustering with Jaccard index}

		As a first clustering method, we clustered structurally equivalent nodes as in \citet{Yodzis1999}, using the Jaccard similarity index as a measure of structural equivalence. See Appendix \ref{appendix:jaccard} for the clustering algorithm.

	\subsection*{Hierarchical clustering with REGE index}

		As a second clustering method, we clustered regularly equivalent nodes as in \citet{Luczkovich2003}, using REGE index as a measure of regular equivalence. See Appendix \ref{appendix:rege} for the clustering algorithm.

	\subsection*{Clustering of density-based modules}

		As a third clustering method, we clustered the nodes inside the modules found by maximising the density-modularity, as in \citet{Guimera2010}. This type of modularity is expressed as the number of extra links present within the modules compared to the ones expected by chance. For directed networks, it can be expressed through the following equation of \citet{Arenas2007}, which is a generalisation of the Newman-Girvan modularity \citep{Newman2004}:

						\begin{equation}
							Q=\frac{1}{L}\sum\limits_{ij}[A_{ij}-\frac{k_i^{in}k_j^{out}}{L}]\delta_{m_im_j} \label{eqn:modularitydensity}
						\end{equation}

		\noindent where $Q$ is the directed modularity of partition P, $L$ is the number of links in the network, $A_{ij}$ is the element of the adjacency matrix of a directed, binary network (links go from $j$ to $i$), $k_i^{in}k_j^{out}/L$ is the probability of having an edge between $i$ and $j$, $k^{in}_i$ is the indegree of $i$ and $k^{out}_j$ is the out-degree of $j$), $ m_i$ is the module of $i$, and $\delta$ is the Kronecker delta \citep{Kozen2007}.

		The number and composition of the modules were found by using the Leiden algorithm of \citet{Traag2019}. This algorithm is an extension of the Louvain algorithm \citep{Blondel2008}. The latter is one of the best performing and fastes for community detection \citep{Traag2019}. However, it tends to produce communities that are arbitrarly poorly connected from each other and sometimes even disconnected. The Leiden algorithm not only solves this problem by producing better connected communities but it is also faster. The code that we used was implemented in the igraph package \citep{Csardi2006} for R \citep{RDevelopmentCoreTeam2011}.

	\subsection*{Clustering of prey-based and predator-based modules}

		As fourth and fifth clustering methods, we clustered the nodes of every module that was found by maximising the prey-modularity and the predator-modularity of the food web, as in \citet{Guimera2010}. In this case, the modularity of the food web is expressed as to how much different nodes connect to the same predators (for prey-modularity) or preys (for predator-modularity) than expected by chance. Mathematically, it can be expressed by the following equation \citep{Guimera2007} for prey-modularity

						\begin{equation}
							Q=\sum_{ij}{\left[\frac{c_{ij}^{out}}{\sum_{l}{k_l^{in}\left(k_l^{in}-1\right)}}-\frac{k_i^{out}k_j^{out}}{\left(\sum_{l} k_l^{in}\right)^2}\right]\delta_{m_im_j}}
						\end{equation}

		or in the following one for predator-modularity

						\begin{equation}
							Q=\sum_{ij}{\left[\frac{c_{ij}^{in}}{\sum_{l}{k_l^{out}\left(k_l^{out}-1\right)}}-\frac{k_i^{in}k_j^{in}}{\left(\sum_{l} k_l^{out}\right)^2}\right]\delta_{m_im_j}}
						\end{equation}

		\noindent where $c_{ij}^{out}$ is the number of outgoing links that i and j have in common and $c_{ij}^{in}$ is the number of incoming links that i and j have in common.
		We maximised this type of modules by using the rnetcarto package \citep{Doulcier2015} for R. This finds the community structure of the network by using simulated annealing \citep{Kirkpatrick1983}.  %CHANGE

	\subsection*{Clustering of groups}

		As a sixth clustering method, we clustered the nodes inside the modules found by the group model of \citep{Allesina2009a}. This model finds the modules that maximise the probability of randomly retrieving the food web by generating a modular version of an Erdős-Rényi random graph. For an arbitrary number of groups $k$, the probability of retrieving the food web is:

				\begin{equation}
					P(N(S,L)|\vec{p}^{\,})=\prod_{i=1}^k\prod_{j=1}^k p_{ij}^{L_{ij}} (1-p_{ij})^{S_i S_j - L_{ij}}
				\end{equation}

		\noindent where $N(S,L)$ is the food web N with S number of nodes and L number of links,  $\vec{p}^{\,}$ is the vector containing the probabilities of a connection between and within clusters, $p_{ij}$ is the probability that a node inside the group $i$ connects to another node inside the group $j$, $L_{ij}$ is the number of links connecting nodes belonging to the group i to nodes belonging to the group j, $S_i$ is the number of nodes in the cluster i,  and $S_j$ is the number of nodes in the cluster j.

		Because of the high number of possible module arrangments, it is not possible to explore them all. To find the best possible solution that our computation power allows us to find, we used the algorithm of \citet{Sander2015}. This relies on a Metropolis-Coupled Markov Chain Monte Carlo ($MC^3$), also known as parallel tempering\citep{Geyer1991}, with a Gibbs sampler \citep{Yildirim2012}. $MC^3$ can be considered as a Markov chain Monte Carlo (MCMC) with multiple chains running all at once \citep{Sander2015}.

\section*{Methods: connecting the clusters and assigning interaction strength}

	The wiring of the food web followed a similar approach to the one describe in \citet{Martinez1991}. We used five methods to decide whether there was a link between two clusters. The first method produced the maximum connectance and is known as maximum linkage. Here, a cluster had a connection to another cluster if it had at least one link going from one of its nodes to the nodes of the second cluster. The second one produced the minimum connectance and is known as minimum linkage. This time, a cluster was connected to another only if all its nodes had a connection to all the nodes of the other cluster. The other three methods produced an intermediate connectance. They considered a link from a cluster to the other only if at least 25\%, 50\% or 70\% of possible connections from the first cluster to the second were realised.

	The weight of the link was then calculated in four different ways: as the minimum weight, the maximum weight, the mean weight, and the sum of the weights of the links going from the members of the first cluster to the ones of the second cluster.

\section*{Methods: centrality indices}

	For each food web, we calculated the centrality indices before and after the aggregation. The centrality index of a node after the aggregation process equaled the one of its cluster. For example, let's consider a hypothetical aggregation. Before the aggregation, the node "hake" has a degree centrality of 5. Through the  aggregation process, this happens to be aggregated into a fish cluster. The degree centrality of this fish cluster is 8. The degree centrality of hake was 5 before the aggregation and 8 after the aggregation.

	\subsection*{Degree centrality (DC)}

		The degree centrality ($DC$) of a node $i$ is the number of links it has \citep{Wasserman1994}

					\begin{equation}
								DC_i=\sum_{j=1}^{n}A_{ij}
					\end{equation}

		\noindent where $n$ is the number of nodes in the food web, and $A_{ij}$ is the element of the adjacency matrix, after the network has been transformed in a binary undirected one. It can be normalised by dividing it by the total number of possible connections that a node could have \citep{Wasserman1994}

					\begin{equation}
								nDC_i=\frac{DC_i}{n-1}\ \ \
					\end{equation}

		\noindent where $n-1$ is the maximum number of connections the node can have. The minus one shows that a node cannot have a connection to itself.

		Another type of degree centrality that we considered was the weighted degree centrality ($wDC$), often referred to as node strength. Its formula, as well as the formula of its normalised version, are the same as for the non-weighted degree centrality. This time, however, the adjacency matrix is of an undirected weighted network \citep{Fornito2016}.

					\begin{equation}
								WDC_i=\sum_{j=1}^{n}A_{ij}
					\end{equation}

	\subsection*{Closeness centrality (CC)}

		The closeness centrality ($CC$) of a node is the average distance a node from all the others \citep{Wasserman1994}

					\begin{equation}
								CC_i=\frac{1}{\sum\limits_{j=1}^n d(i,j)}
					\end{equation}

		\noindent where $d(i,j)$ is the shortest path between node $i$ and $j$. It can be normalised as follows \citep{Wasserman1994}

					\begin{equation}
								nCC_i=\frac{n-1}{\sum\limits_{j=1}^n d(i,j)}
					\end{equation}

	\subsection*{Betweenness centrality (BC)}

		The betweenness centrality ($BC$) of a node is the average number of times that it acts as a bridge along the shortest path between two other nodes. It can be  mathematically expressed as follows \citep{Wasserman1994}

						\begin{equation}
							BC_i=\sum_{i\neq m\neq n}\frac{\sigma_{mn}\left(i\right)}{\sigma_{mn}}
						\end{equation}

		\noindent where $\sigma_{mn}$ is the total number of shortest paths going from $s$ to $t$ and $\sigma_{mn}\left(i\right)$ is the total number of these paths passing through $i$. It can be normalised with the following equation \citep{Wasserman1994}

						\begin{equation}
							nBC_i=\frac{BC_i}{\left(n-1\right)\left(n-2\right)/2}
						\end{equation}

	\subsection*{Status index (s)}

		The status index of a node is the sum of its distances from all the other nodes inside the network, calculated as their shortest paths following a bottom-up direction \citep{Endredi2018}

						\begin{equation}
							s_i=\sum_{j=1}^{n}d\left(i,j\right).
						\end{equation}

		It was first introduced to social networks, followed two years later by its application to food webs by \citet{Harary1959, Harary1961}. By following the same method but in a top-down direction we obtain the controstatus $(s_i’)$

						\begin{equation}
							s_i^\prime=\sum_{j=1}^{n}d\left(i,j\right).
						\end{equation}

		The difference between the status and the controstatus is called the net status ($\Delta s_i$)

						\begin{equation}
							\Delta s_i=s_i-s_i^\prime.
						\end{equation}

	\subsection*{Keystone index (K)}

		The keystone index was firstly introduced by \citet{Jordan1999} and inspired by the status index. As the status index family, the keystone index of a species $i$ ($K(i)$) is calculated by considering the bottom-up and the top-down effects separately \citet{Jordan2006}

						\begin{equation}
							K\left(i\right)=K_b\left(i\right)+K_t\left(i\right)
						\end{equation}

		\noindent where $K_b\left(i\right)$ is its bottom-up keystone index of species $i$ and $K_t\left(i\right)$ the top-down keystone index of species $i$.

		Unlike the status index, which only considers the distance between a node and all the other nodes, the keystone index takes into consideration how the size of a certain effect gets split between the different neighbours of a node. Every time the effect reaches a certain node connected to multiple nodes, the following nodes receive only a fraction of the total effect. For example, when considering the bottom-up effect, if the prey has two predators, the bottom-up effect received by each predator will be half. The bottom-up effect of a certain node $(K_b\left(i\right))i$ is then calculated in the following way

						\begin{equation}
							K_b\left(i\right)=\sum_{j=1}^{n}\frac{1}{m\left(i\right)\left(j\right)}+\frac{K_b\left(j\right)}{m\left(i\right)\left(j\right)}
						\end{equation}

		\noindent where $j$ is a predator of $I$,$m(i)(j)$ is the number of preys of $j$, and $\frac{K_b\left(j\right)}{m\left(i\right)\left(j\right)}$ is the fraction of bottom-up effects of $j$ that are caused by $i$. The $K_b\left(j\right)$of top predators is set as 0. The top-down effect of a certain node $K_t\left(i\right)$ is calculated exactly as $K_b\left(i\right)$, but with the direction of the links inverted. The bottom-up and the top-down effects can also be split into their direct and indirect component. The indirect component takes into consideration the bottom-up effects of the predator and direct component does not

					\begin{equation}
						K_{b,indirect}\left(i\right)=\sum_{j=1}^{n}\frac{1}{m\left(i\right)\left(j\right)}+\frac{K_b\left(j\right)}{m\left(i\right)\left(j\right)}
					\end{equation}

					\begin{equation}
						K_{b,direct}\left(i\right)=\sum_{j=1}^{n}\frac{1}{m\left(i\right)\left(j\right)}+\frac{1}{m\left(i\right)\left(j\right)}
					\end{equation}

		The direct and indirect components of the top-down effect are calculated in the same way, but with the direction of the links inverted. The direct and indirect keystone indices of a node are the sum of its direct/indirect bottom-up effects and its direct/indirect top-down effects

					\begin{equation}
						K_{direct}(i)=K_{b,direct}+K_{t,direct}
					\end{equation}

					\begin{equation}
						K_{indirect}(i)=K_{b,indirect}+K_{t,indirect}
					\end{equation}

		The keystone index not only is the sum of its top-down and bottom-up effects, but also the sum of its direct and indirect effects

					\begin{equation}
						K\left(i\right)=K_{dir}\left(i\right)+K_{indir}\left(i\right)
					\end{equation}

	\subsection*{Topological importance (TI)}

		The topological importance of a node represents its potential to create bottom-up effects on other species, up to a certain number of steps that we can set. It was first introduced to host-parasitoid networks by \citet{Muller1999} and then to food webs by \citet{Jordan2003}. The algorithm of its computation is reported in Appendix \ref{appendix:TI} \citep{Jordan2009}.



		Topological importance can be also used for weighted networks - giving us weighted topological importance ($WI$) – if instead of using the degree ($D$) we use the weighted degree ($WD$) \citep{Scotti2007}

						\begin{equation}
							a_{1,ji}=\frac{A_{ij}}{weighted\:indegree_j}
						\end{equation}

		\noindent where $A_{ij}$ is the element of the adjacency matrix of the weighted directed network.

	\subsection*{Trophic field overlap (TO) & species uniqueness (STO)}

		The trophic field overlap (TO) represents how redundant the strong interactions of a node are. It was first introduced by \citet{Jordan2009a}. It is the number of times that it and another node interact strongly with the same predator. The algorithm for its computation can be found in Appendix \ref{appendix:TO} \citep{Jordan2018}.


		Trophic field overlap can be also used for weighted networks – giving us weighted trophic field overlap (WO) – if instead of using the degree (D) we use the weighted degree, (e.g., \citet{Xiao2019})

						\begin{equation}
							a_{1,ij}=\frac{A_{ij}}{D_j}
						\end{equation}

		Finally, to avoid the bias of choosing a wrong threshold, we chose multiple thresholds and summed the TO of a species i for each of these thresholds. This gave us the species uniqueness (STO), an index that was firstly introduced by \citet{Lai2015}.

	\subsection*{Trophic position (TP)}

		The trophic position of a node is the mean length connecting it to the producers of the ecological community (its energy source). It was firstly introduced by \citet{Levine1980}, as a generalization of the earlier use of integer trophic levels to include fractional positions. It can be calculated through the following formula

						\begin{equation}
							TP_i=\sum\limits_{k=0}^\infty k \cdot p_i(k).
						\end{equation}

		\noindent where $k$ is a certain path length and $p_i(k)$ is the probability that species $i$ will reach the energy produced by the autotrophs via a path of length $k$. $TP$ equals 0 for producers, it equals 1 for herbivores and larger values for omnivores and carnivores.

\section*{Methods: statistical analysis}

	The combination of clusterings (6 methods), linkages (5 methods) and interaction strength determinations (4 methods) produced 120 aggregation methods. For each of these aggregation methods, we studied their effects on centrality indices. More in particular, for each centrality index we studied how the nodes were ranked before and after the process. It was possible to study the difference in the these two rankings by using Kendall's tau b ($\tau_B$) - a version of Kendall's rank correlation coefficient that makes adjustments for ties \citep{Agresti2012}. For each aggregation method and for each centrality index, we found the mean $\tau_B$ across all food webs. This required us to convert $\tau_B$ using the Fisher z-transformation \citep{Fisher1915}, find the mean and back-transform it. For each mean $\tau_B$ we found its confidence interval by bootstrapping \citep{DiCiccio1996}. $\tau_B$ and bootstrapping were implemented in the Statistics and Machine Learning Toolbox for MATLAB \citep{MathworksInc.2019}.

\section*{Results}

	\subsection*{Clustering of the food webs}

		The 86 food webs had a median of 25.5 nodes (IQR = 16.0), with a minimum of 14 nodes and a maximum of 55 nodes. See Figure \ref{fig:food_web_sizes}. They produced a median number of 0.76 (IQR = 0.11) for the Jaccard index, 0.73 (IQR = 0.07) for the REGE index, 0.16 (IQR = 0.08) for the density modularity, 0.35 (IQR = 0.02) for the prey modularity, 0.16 (IQR = 0.08) for the predator modularity and 0.16 (IQR = 0.07) for the group model. See Figure \ref{fig:cluster_sizes}.

						\begin{figure}[htbp]%{\textwidth}
								\centering
								\includegraphics[width=1.0 \linewidth]{food_web_sizes.jpg}
								\caption{Size of the 86 food webs used in this study.}
								\label{fig:food_web_sizes}
						\end{figure}

						\begin{figure}[htbp]%{\textwidth}
								\centering
								\includegraphics[width=1.0\linewidth]{cluster_sizes.jpg}
								\caption{Number of clusters produced by the different aggregation methods. (a) = hierarchical clustering with Jaccard index, (b) = hierarchical clustering with REGE index, (c) = density-based modules, (d) = prey-based modules, (e) = predator-based modules, (f) = groups.}
								\label{fig:cluster_sizes}
							\end{figure}

	\subsection*{Comparison between the original and the clustered rankings}

	The results of the comparison between the ranking of nodes in the original and the clustered food webs can be seen in Figure \ref{fig:jaccard_results} - \ref{fig:groups_results}.

\section*{Discussion}

	When we have large food webs, we need to aggregate them to better understand them, especially if we want to simulate them dynamically. The best way of comparing different aggregation methods would be to check which one is the one that maintains the dynamics the best. However, we did not have enough time to see what is the best one in that sense. This is why we decided to use structure as a proxy for dynamics. We used centrality indices as a proxy for network structure. Centrality indices tell us about many things of nodes. This is why we used them, as a proxy for structure. So we can actually say that this is a study where we check whether large food webs can be aggregated into smaller ones by maintaining the properties of the nodes. 

	Different centrality indices have different predictive power to find keystone species. The most reliable indices (e.g. keystonenness index, see \citep{Libralato2006}) are the ones based on a dynamical model of the network. However, building a good dynamical model is really time consuming and might not be feasible for multiple networks.	This is why often researchers use topological indices, such as the ones studied in this work. To check the reliability of topological indices, \citet{Gouveia2020} tested how much they aggreed with the dynamical index keystoneness (KS).	They found that the most reliable topological index was the weighted degree (wDC). It could predict the most important species 70.06\% of the times. It was followed by the 5-step weighted topological importance (WI5).	A combination of wDC and WI5 increased this percentage to 78.42\%. It seems like a good aggregation should then maintain these two indices. Another index that a good aggregation method should maintain is the trophic level. This can be calculated, for example, as the trophic position in this study. Trophic level is associated with many biological properties of species. For example, the direction by which a perturbation propagates to \citep{Curtsdotter2011}.	Or the risk of extinction of a species \citep{Binzer2011}. Or the success of a species to colonise a new environment \citep{Holt2010}. (Add: \citet{Gouveia2020} didn't take STO and wSTO into consideration, as well as the trophic level.)

	By looking at the results in Figure \ref{fig:kendall_results}, we see clear differences between algorithms. Density modularity is the worst aggregation algorithm. This is probably because it produces communities based on other factors (such as phylogeny, body mass, and habitat structure, see \citet{Rezende2009}) rather than trophic links between modules. Predator modularity, prey modularity and the group model also performed poorly, considering they performed worse than REGE and Jaccard across all centrality indices (except for predator modularity that performed the best with regards to trophic position). Jaccard and REGE seem to be the best aggregation methods.	They both maintained well the rankings of all centrality indices.	Jaccard maintained the most number of centrality indices slightly better than REGE.	However, REGE maintained better the pattern of the two most important indices according to \citet{Gouveia2020}, wDC and WI5. In light of these findings, we suggest to use either the Jaccard index or the REGE index for aggregation in keystone species research. The clusters produced by these two similarity indices can be useful to answer different questions. The Jaccard index maintains all the information about a certain network, but it doesn't allow you to compare different food webs \citep{Luczkovich2003}. The REGE index produces a controlled loss of information but it allows to compare between different food webs \citep{Luczkovich2003}.

	Future work should be focused on developing new algorithms for the aggregation of species in food webs. It should follow five steps:
		\begin{enumerate}
			\item Developing a singular index that tells us the goodness of the aggregation.
			\item Deciding what is the best method to assign the interaction strength of the link between the clusters.
			\item Deciding what is the best percentage of realised links between clusters to decide that there is a link between them.
			\item Deciding what are the indices we should use. Are more indices better or worse? Should we combine all indices? Or should we develop different aggregation methods according to the type of pattern we want to preserve (e.g., redundancy, capacity of spreading an effect)?
		\end{enumerate}
	This research, however, is complicated by our lack of understanding of keystone species. It is not clear what the best centrality indices are. Also it seems like trophic links might not be the only interactions important in determining keystone species.	\citet{Donohue2017} found that secondary extinctions were better modelled when they took into consideration competition.	This might mean that a multilayer approach (see \citet{Pilosof2017}) could be a better way of finding keystone species.	So we would argue that developing algorithms of aggregation should go hand in hand with this type of research. Another possible direction is checking how different data aggregations influence dynamical indices instead of topological indices. For example, the keystonenness index (KS) \citep{Libralato2006} or eigenvector centrality \citep{Allesina2009}.

	Another possible future direction is using new algorithms for clustering.	These could be hierarchical clustering with different types of similarity indices (e.g., automorphic equivalence \citep{Wasserman1994}, Katz similarity \citep{Newman2018}). Or they could be new algorithms of directed network clusterings (see the comprehensive review of \citet{Malliaros2013}). For example, \citet{Zhou2005,Huang2006,Wang2008,Kim2010} and \citet{Zhan2011} have never been cited by the ecological literature (according to Google Scholar) and their application to food webs might reveal to be useful.

	An interesting way of using aggregation, however, could be to find keystone species.
	It has been suggested by \citep{Bond1994} that keystone species might be the species that cannot be aggregated.
	This makes us wonder whether it would be possible to use aggregation to find keystone species in topological networks.
	And whether they would work better than centrality indices.

	We always need to remind ourselves that aggregated food webs bias our analyses.
	Ultimately, the question should be also whether we should aggregate our data.
	Aggregating entities in ecology has been found to always bias models and produce worse results.
	For example, \citet{Woodward2010b} showed how using the Allometric Diet Breadth Model (ADBM) \citep{Petchey2008} to predict the links between nodes predicted 52\% of the links when building a species-based food web.
	This number increased to 83\% when considering size-based classes.
	Another example is from \citet{Petchey2002}.
	When species were put into functional groups, the community showed high functional redundancy.
	When single species were considered singularly, the community showed low functional redundancy.
	We might argue that the best way of working with food webs is not aggregating. However, the problem is that it is never possible to do this.
	This is because we have a limited resolution power when we sample and the data that have been collected in the past and are avaible for our analyses have low resolution, at least in some part of the network.
	This could be solved by the way we sample.
	Ideally, we should develop technologies and methods that allow us to sample a food web in its highest resolution possible.
	However, this might be really expensive and ecology might not have enough money to finance this type of research.
	This should be especially directed toward charachtering the lowest trophic levels of the food web, for example plankton in aquatic food webs.
	Aggregation research should also inform sampling methods and how to deal with missing data (see \citet{Patonai2017}).

	It is also important to remember that it seems like species who are unique trophospecies seem to be really important for secondary extinction. In particular, \citep{Petchey2008a} found that they are particularly vulnerable to secondary extinctions when trying to model their dynamic food web. The fact that the concept of trophospecies not only is important to understand which ones are the most vulnerable species in the system, but it seems also important in something related to how different species are related to each other. If, as someone said but I don't remember who, keystone species are the ones that are unique in their trophospecies, it means that secondary extinctions happen only if you hit the network close or on keystone species. This is because keystone species, if my interpretation is correct, are not only the most important, but also the most vulnerable. This might be due to the fact that they have such low abundance as well.
	Keystone species might be also the ones that we cannot aggregate in a food web \citep{Bond1994}. So, at this point, maybe data aggregation would reveal keystone species. The fact that the aggregation of according to Jaccard not only can reveal the keystone species, but also would maintain the relative importance of the nodes, seems like a great way of aggregating data. Wait: I need to check whether the species that are unique in their cluster also have high centrality indices.

\section*{Acknowledgements}

	We would like to thank Wei-Chung Liu for providing the code for computing some centrality indices and Stefano Allesina \& Elizabeth Sander for providing the code for the  computation of the group model. Ferenc Jordàn was supported by H2020 AtlantECO.

\section*{Supplementary material}

	The adjacency matrix of the food web of the Gulf of Naples, as well as the code used to analyse it is available at \url{https://github.com/Emanuele-Giacomuzzo/Data_aggregation}.

\bibliographystyle{apalike}
\bibliography{/Applications/Mendeley/library.bib}

\onecolumn
\begin{appendices}

	\section{Hierarchical clustering with Jaccard index} \label{appendix:jaccard}

		\begin{enumerate}

			\item \emph{Compute similarity.} \smallskip \newline
						Compute the Jaccard similarity between the nodes by using the following equation \citep{Yodzis1999}:

										\begin{equation}
				      				J_{ij}=\frac{a}{a+b+c} \label{eqn:jaccard}
			      				\end{equation}

			      \noindent where $J_{ij}$ is the Jaccard similarity between node $i$ and $j$, $a$ is the number of preys and predators that $i$ and $j$ have in common, $b$ is the number of preys and predators exclusively of i, and $c$ is the number of preys and predators exclusively of $j$.

			\item \emph{Build the dendrogram.} \smallskip \newline
			      Find the two most similar elements and cluster them together (elements are intended as nodes or clusters. Of course, the first time we run this step all the elements are nodes). Repeat until you are left with only one item, which is the final dendrogram. During this process, the similarity between two clusters can be calculated in different ways, called linkage criteria. The ones that we used were

						\begin{itemize}
				      \item 	The similarity between the least similar nodes, one in each cluster, known as single-linkage \citep{Frigui2008}.
				      \item 	The similarity between the most similar nodes, one in each cluster, known as complete linkage \citep{Frigui2008}.
				      \item 	The mean similarity between the nodes inside the first item and the second item, known as the weighted average distance(WPGMA) \citep{Sokal1958}:

				            	\begin{equation}
					            	d_{(i \bigcup j),k}=\frac{d_{i,k}+d_{j,k}}{2} \label{eqn:WPGMA}
				            	\end{equation}

											\noindent where $d_{\left(i\cup j\right),k}$ is the distance between the cluster $i \bigcup j$ (cluster including $i$ and $j$) and $k$, $d_{i,k}$ is the distance between $i$ and $k$, and  $d_{j,k}$ is the distance between $j$ and $k$.
				      \item	The mean similarity between the nodes inside the first item and the second item, but taking into consideration the average distance between the items inside the fist cluster; this is known as the unweighted average distance (UPGMA) \citep{Sokal1958}:

				            	\begin{equation}
					            	d_{(i \bigcup j),k}=\frac{|i|d_{i,k}+|j|d_{j,k}}{|i|+|j|} \label{eqn:UPGMA}
				            	\end{equation}

							\noindent where $|i|$ and $|j|$ are the mean distances between the elements inside $i$ and $j$, respectively.
			      \end{itemize}

			\item \emph{Select the dendrogram.} \smallskip \newline
						After having produced a dendrogram for every linkage criteria, select the dendrogram with the highest cophenetic correlation \citep{Sokal1962}.
			      This allows selecting the linkage criterion that produces the dendrogram that preserves the most faithfully the pairwise similarity between different elements.

			\item \emph{Cut the dendrogram.} \smallskip \newline
			      Cut the dendrogram according to the maximum inconsistency of the branches, set at 0.01.

		\end{enumerate}

	\section{Hierarchical clustering with REGE index} \label{appendix:rege}

		\begin{enumerate}

			\item \emph{Compute similarity.} \smallskip \newline
			Compute the similarity between nodes by using REGE, calculated by the homonym algorithm. This was originally developed in the unpublished work by \citet{White1980,White1982,White1984} and firstly described in the literature by \citet{Borgatti1993}. It is available to be used in the software UCINET VI \citet{Borgatti2002}. The REGE algorithm is as follows \citep{Jordan2018}:

				      \begin{enumerate}

					      \item Set the maximum number of iterations. We set 3 iterations. Each iteration produces a matrix $R_{\left(t\right)}$ where $t$ is the number of the iteration and every element $r_{\left(t\right)ij}$ is the regular equivalence between i and j at iteration t. The regular equivalence between nodes at iteration t=0 is always 1.

					      \item Starting from t=1, update the elements of the matrix following these sub-steps:

					            \begin{enumerate}
						            \item For every predator k of species i, find the most similar predator m of species j according to  $R_{\left(t\right)}$.
						                  Now, set $X_{i,k,j}=R_{\left(t\right)km}.$
						            \item For every predator m of species j, find the most similar predator k of species o according to $R_{\left(t\right)}$.
						                  Now, set $X_{j,m,i}=R_{\left(t\right)mk}$.
						            \item For every prey h of species i, find the most similar prey n of species j according to $R_{\left(t\right)}$.
						                  Now, set $Y_{i,h,j}=R_{\left(t\right)hn}$.
						            \item For every prey n of species j, find the most similar prey h of species i according to $R_{\left(t\right)}$.
						                  Now, set $Y_{j,n,i}=R_{\left(t\right)nh}$.
						            \item Update the matrix R through the following equation
						                  %\begin{equation}
						                  % R_{\left(t\right)ij}=\frac{\sum_{k=1} X_{i,k,j}+\sum_{m=1} X_{j,m,i}+\sum_{h=1} Y_{i,h,j}+\sum_{n=1} Y_{j,n,i}}
						                  %{MAX\left(\sum_{k=1} X_{i,k,j}+\sum_{m=1} X_{j,m,i}+\sum_{h=1} Y_{i,h,j}+\sum_{n=1} Y_{j,n,i}\right)}
						                  %\end{equation}
						            \item Increase t=t+1 and repeat step b until you reach the maximum number of iterations. The matrix of the maximum number of iterations contains the regular equivalence between nodes.
					            \end{enumerate}

					      \item Increase t=t+1 and repeat step b until you reach the maximum number of iterations. The matrix of the maximum number of iterations contains the regular equivalence between nodes.

				      \end{enumerate}

			\item \emph{Build the dendrogram.} \smallskip \
			The same as in the hierarchical clustering of nodes according to their Jaccard similarity index. During our analysis, we used the function linkage of MATLAB, which does not include the possibility of using a similarity matrix, so we converted the similarity matrices into dissimilarity ones. This was done by following what was written in \citet{VonLuxburg2004}. Namely, if the similarity function is normalised - takes values between 0 and 1 - and always positive, then $d=1-s$ where d is the dissimilarity measure and s is the similarity measure).

			\item \emph{Select the dendrogram.} \smallskip \newline
			The same as in the hierarchical clustering of nodes according to their Jaccard similarity index.

			\item \emph{Cut the dendrogram.} \smallskip \newline
			The same as in the hierarchical clustering of nodes according to their Jaccard similarity index.

		\end{enumerate}

	\section{Topological importance computation} \label{appendix:TI}

		\begin{enumerate}

			\item \emph{Compute the one-step matrix.} \smallskip \newline
			In the one-step matrix, if the energy flows from a prey to the predator, then the effect of the prey on the predator is the reciprocal of the indegree of the predator

								\begin{equation}
									a_{1,ij}=\frac{A_{ij}}{D_j}
								\end{equation}

			\item \emph{Compute the n-step matrices.} \smallskip \newline
			In the higher steps matrices, a node influences another node at a higher trophic level by summing the effects of every path that connects the two nodes. The effect of every path is the multiplication of the inverse of the outdegree of every node along the path. For a visual explanation see Figure \ref{fig:TI}. It can be calculated as follows

								\begin{equation}
									A\left(n\right)=A_{\left(1\right)}^n
								\end{equation}

			\item \emph{Calculate topological importance} \smallskip \newline
			The topological importance of a node i ($TI_i$) can be calculated through the following formula

								\begin{equation}
									TI_i=\frac{\sum\limits^N_{m=1}\sum\limits^n_{j=1}a_{m,ji}}{N}
								\end{equation}

			\noindent where $N$ is the total number of steps considered, $m$ is the step number,  $n$ is the total number of nodes, and $a_{m,ji}$ is the effect of species $i$ on species $j$ at $m$ number of steps.

								\begin{figure}[htbp]%{\textwidth}
									\centering
									\includegraphics[width=1.0\linewidth]{TI_example.png}
									\caption{Topological importance (TI) of a species on another. The plant and the bear are not connected, so there is no direct effect from the plant to the bear. However, indirect effects reach the bear from the plant through two paths and the final effect is the sum of these effects. The first one is through the deer and the second is through the fox. The strength of these paths is the product of the direct effects composing the path. The first path has an effect on the bear that is 0.9*0.6=0.54, the second one has an effect on the bear that is 0.1*0.4=0.04. Summing the effects through these two 2-step paths connecting the plant with the bear, we get the 2-step effect of the plant on the bear: 0.54+0.04=0.58. Figure created with BioRender.com.} \label{fig:TI}
								\end{figure}

		\end{enumerate}

	\section{Trophic field overlap computation} \label{appendix:TO}

		\begin{enumerate}
			\item Compute the one-step matrix as in toplogical importance \item Compute the n-step matrix as in topological importance \item Compute the average effect matrix. The average effect matrix ($E(n)$) represents the effect of each node on the other nodes average by the number of steps

					\begin{equation}
						E_n=\frac{1}{n}\sum_{i=1}^{n}A_{\left(i\right)}
					\end{equation}

			\item Compute the interactor matrix.
					Compute the so-called interactor matrix ($M_T$), whose values tell us whether the interaction between two nodes is weak (W) or strong (S).
					To do this, we need to define a threshold over which a certain interaction is strong.
		  \item Compute the topological overlap matrix.
					Compute a matrix with how many times two species interact strongly with the same predator, called the topological overlap matrix.
		  \item Compute the trophic field overlap (TO)
					The trophic field overlap (TO) of a node is calculated by summing the elements of the rows of the topological overlap matrix.
		\end{enumerate}

	\section{Possible results}

		\begin{figure}[b]%{\textwidth}
			\centering
			\includegraphics[width=1.0 \textwidth]{heatmap_jaccard.jpg}
			\caption{Results of the comparison between the ranking before and after the aggregation through the Kendall's tau b coefficient, also known as the Kendall's rank correlation coefficient. The results are reported as median and interquartile range.} \label{fig:jaccard_results}
		\end{figure}

		\begin{figure}[b]%{\textwidth}
			\centering
			\includegraphics[width=1.0 \textwidth]{heatmap_rege.jpg}
			\caption{Results of the comparison between the ranking before and after the aggregation through the Kendall's tau b coefficient, also known as the Kendall's rank correlation coefficient. The results are reported as median and interquartile range.} \label{fig:rege_results}
		\end{figure}

		\begin{figure}[b]%{\textwidth}
			\centering
			\includegraphics[width=1.0 \textwidth]{heatmap_density.jpg}
			\caption{Results of the comparison between the ranking before and after the aggregation through the Kendall's tau b coefficient, also known as the Kendall's rank correlation coefficient. The results are reported as median and interquartile range.} \label{fig:density_results}
		\end{figure}

		\begin{figure}[b]%{\textwidth}
			\centering
			\includegraphics[width=1.0 \textwidth]{heatmap_predators.jpg}
			\caption{Results of the comparison between the ranking before and after the aggregation through the Kendall's tau b coefficient, also known as the Kendall's rank correlation coefficient. The results are reported as median and interquartile range.} \label{fig:predators_results}
		\end{figure}

		\begin{figure}[b]%{\textwidth}
			\centering
			\includegraphics[width=1.0 \textwidth]{heatmap_preys.jpg}
			\caption{Results of the comparison between the ranking before and after the aggregation through the Kendall's tau b coefficient, also known as the Kendall's rank correlation coefficient. The results are reported as median and interquartile range.} \label{fig:preys_results}
		\end{figure}

		\begin{figure}[b]%{\textwidth}
			\centering
			\includegraphics[width=1.0 \textwidth]{heatmap_groups.jpg}
			\caption{Results of the comparison between the ranking before and after the aggregation through the Kendall's tau b coefficient, also known as the Kendall's rank correlation coefficient. The results are reported as median and interquartile range.} \label{fig:groups_results}
		\end{figure}


	\section{Table with food webs} \label{appendix:food_webs}

		\begin{center}
			\begin{tabular}{ c c c c }
				\textbf{Food web} & \textbf{Number of nodes} & \textbf{Connectance} & \textbf{Reference} \\
				\hline
				Great Barrier Reef-prawn & 23 & Connectance & Reference \\
			9 Baltic Sea (9BS)	15
			Baltic Sea model in NEST (BSN)	15
			16 BRUNEI	12
			25 Maspalomas Lagoon	5
			27 Central Chile 1992	20
			28 Tongoy Bay	16
			33 Gulf of Nicoya	20
			34 Golfo Dulce	20
			38 Bay of Somme	8
			39 THAU	10
			40 Gironde estuary	16
			42 Seine Estuary	14
			47 Sakumo Lagoon	12
			52 Iceland's fisheries	20
			53 Iceland - 1950	23
			58 Lagoon of Venice	15
			60 Orbetello Lagoon (60OL)	11
			Orbetello 1995 (O95)	11
			70 La Paz Bay	21
			71 Central Gulf of California	25
			73 Sonda de Campeche	18
			76 Huizache-Caimanero	25
			77 Alto Golfo de California	28
			79 Terminos Lagoon	19
			80 MANDINGA	19
			81 Tampamachoco Lagoon	22
			83 CELESTUN	15
			87 MAPUTO	9
			89 Loyalty Islands Atoll	23
			93 Aleutians 2005 for web	38
			106 Cantabrian Sea	26
			108 Kuosheng Bay	15
			109 Lagoon Chiku - Taiwan	12
			113 English Channel 1995	48
			114 MONTEREY	15
			116 USA, West Florida Shelf	55
			118 Western Bering Sea s+s	33
			119 Prince William Sound	45
			120 Prince William Sound_old	18
			122 LOOE NEW	19
			123 VENEZUELA	15
			131SeagrassMangroveTerminos	14
			135 North Sea_1880	44
			138 West Scotland	36
			139 Western English Chanel	50
			143 Bay of Bengal	14
			144 Southwest coast of India	10
			147 W coast of Sarawak	28
			148 W coast of Sabah	28
			149 San Pedro Bay, Leyte	15
			152 Southern Brazil	12
			153 Strait of Georgia	26
			161 South Brazil Bight	22
			171 GOMEXICO	14
			174 WGMEXICO	23
			175 TAMIAHUA	12
			177 YUCATAN	20
			180 CAMPECHE	18
			181 VIRGIN ISLANDS	20
			Adriatic90 (A90)	36
			AegeanSea	38
			Arabian Sea Karnataka	23
			Bay Calvi	26
			CAPE VERDE	30
			CuronianLagoon	9
			Deep WC Scotland	35
			East China Sea	42
			EScotianShelf80 (ES80)	38
			EScotianShelf90 (ES90)	38
			GAMBIA	21
			Gamtoos estuary	28
			GUINEA	43
			GUINEABISSAU	30
			Gulf of Riga	11
			Gulf of Thailand	38
			Kromme estuary	29
			LithuanianCoast	9
			MAURITANIA	37
			Miramare_Natural_Mar_Reserve	17
			MOROCCO	36
			NGulf of California	33
			NGulfStLaurence2000 (NGSL2000)	30
			North Sea 1991	63
			Northern Benguela 1956	31
			Parnu Bay	11
			Puck Bay	11
			San Miguel Bay	18
			SCatSea1980s (SCS80)	37
			SCatSea1990s (SCS90)	37
			SCatSea2003 (SCS2003)	36
			SENEGAMBIA	16
			SGulfStLaurence80 (SGSL80)	29
			SGulfStLaurence90 (SGSL90)	29
			Sierra Leone 1964 (SL64)	43
			Sierra Leone 1978 (SL78)	43
			Sierra Leone 1990 (SL90)	43
			Sorfjord	26
			Sundays estuary	28
			Swartkops estuary	30
			Venice Lagoon_Seagrass (VL1S)	24
			Venice Lagoon_Tapes (VL2T)	25
		\end{tabular}
	\end{center}

\end{appendices}
\end{document}
