#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
@author: Catarina Gouveia
"""

# Importing the libraries
import pandas as pd
from gplearn.genetic import SymbolicRegressor
from sklearn.model_selection import KFold
import random
from scipy.stats import spearmanr
from numpy.random import RandomState

# Allows the results to be reproducible
random_state = RandomState(seed = 201819)

# Importing the dataset:
path = r"C:\ " # Define the path

# CHANGE THIS - realvalues / rankorder
data = pd.read_csv(path+r"totaldata_realvalues.csv")

# Choosing our data features and matrix of target variable
X = data[data.columns[:-1]] #1st to 18th are our features
y = data[data.columns[-1:]] #last column is our target variable
y_vector = data[data.columns[-1:]].values.ravel()

var = {}
results = {}
i = 0

for j in range(10000):
    
    population_size = 1000
    
    # Atributing random values to the next 4 different variables:
    p_crossover = round(random.uniform(0.5, 0.9), 1)
    p_hoist_mutation = round(random.uniform(0.01, 0.1), 2)
    p_point_mutation = round(random.uniform(0.01, 0.1 - p_hoist_mutation), 2)
    #p_hoist_mutation + p_point_mutation can't exceed 0.1 - otherwise can cause trouble
    p_subtree_mutation = abs(round(random.uniform(0.01, 1 - p_crossover - p_hoist_mutation -
                                                  p_point_mutation), 2))
    
    if p_crossover + p_hoist_mutation + p_point_mutation + p_subtree_mutation > 1:
        p_hoist_mutation = 0
    results[j] = {}
    print(j)
    
    # Splitting the data in 10 folds
    kf = KFold(n_splits = 10, random_state = random_state, shuffle = False)
    
    
    while population_size <= 10000:
        all_est_gp = []
        for train_index, test_index in kf.split(data):
            X_train, X_test = X.iloc[train_index], X.iloc[test_index]
            y_train, y_test = y.iloc[train_index].values.ravel(), y.iloc[test_index].values.ravel()
            # This X_train and y_train will be used to fit and generate the model

            # gplearn model
            est_gp = SymbolicRegressor(generations = 100,
                                           p_crossover = p_crossover,
                                           p_hoist_mutation = p_hoist_mutation,
                                           p_point_mutation = p_point_mutation,
                                           p_subtree_mutation = p_subtree_mutation,
                                           population_size = population_size,
                                           function_set = ('add', 'sub', 'mul', 'div'),
                                           tournament_size = 0.5 * population_size,
                                           n_jobs = -1,
                                           low_memory = True,
                                           metric = 'spearman',
                                           parsimony_coefficient = 'auto',
                                           random_state = random_state
                                           )
            est_gp.fit(X_train, y_train)
            
            # New line to append the 10 different solutions to each subset
            all_est_gp.append((est_gp.__str__(), str(est_gp._program.raw_fitness_)))
            to_calc_test_averageTest = []
            to_calc_test_averageTrain = []
            to_calc_test_averagepTest = []
            to_calc_test_averagepTrain = []
            
            pred3 = est_gp.predict(X_train)
            
            ##################################################
            #Correlation: train data
            # (Must be the same of the one that comes out from the algorithm)
            scoreTrain_Algorithm, pval_algorithm = spearmanr(y_train, pred3) 
            ##################################################
            
            
            # Calculating predictions using X_test, X_train (should be exactly the same
            # than the real ones) and with all the data, X (the predictions coming out from the
            # ones that are not used to train should be different)
            pred = est_gp.predict(X_test) # est_gp is your trained gp
            pred2 = est_gp.predict(X_train) # Used as control, to verify
            pred_alldata = est_gp.predict(X) # Used as control, to verify
            
            
            ##################################################
            
            scoreTrain, pTrain = spearmanr(y_train, pred2) # Used as control, to verify
            #pTrain contains p-values
            scoreTest, pTest = spearmanr(y_test, pred)
            score_alldata, p_alldata = spearmanr(y_vector, pred_alldata) # Used as control, to verify
            
            ##################################################
            
            
            all_est_gp.append((scoreTrain, pTrain))
            all_est_gp.append((scoreTest, pTest))
            all_est_gp.append((score_alldata, p_alldata))
            
           
            # Save results in dictionary format
            
            results[j]['p_crossover'] = est_gp.p_crossover
            results[j]['p_hoist_mutation'] = est_gp.p_hoist_mutation
            results[j]['p_point_mutation'] = est_gp.p_point_mutation
            results[j]['p_subtree_mutation'] = est_gp.p_subtree_mutation
            #results[j]['random_state'] = est_gp.random_state.get_state()
            results[j]['population size: '+str(est_gp.population_size)] = {}
            results[j]['population size: '+str(est_gp.population_size)] = [all_est_gp]
            
            for train_index, test_index in kf.split(data):
                X_train, X_test = X.iloc[train_index], X.iloc[test_index]
                y_train, y_test = y.iloc[train_index].values.ravel(), y.iloc[test_index].values.ravel()
                    

                to_calc_test_averageTest.append(scoreTest)
                to_calc_test_averageTrain.append(scoreTrain)
                
                to_calc_test_averagepTest.append(pTest)
                to_calc_test_averagepTrain.append(pTrain)
                
    averageTest = sum(to_calc_test_averageTest)/10
    averagepTest = sum(to_calc_test_averagepTest)/10
    averageTrain = sum(to_calc_test_averageTrain)/10
    averagepTrain = sum(to_calc_test_averagepTrain)/10
     
    # This X_train and y_train will be used to fit and generate the model
    results[j]['population size: '+str(est_gp.population_size)].append(
    
    [ # Algorithm fitness [est_gp.__str__(), str(est_gp._program.raw_fitness_), #Algorithm fitness
    (scoreTrain_Algorithm, pval_algorithm), # It has to be the same of the one from the algorithm
    (to_calc_test_averageTest[0], to_calc_test_averagepTest[0]),
     
    # The Spearman that comes out directly from the data test
    # (the only data not used to produce the model)
    (averageTest, averagepTest), # Average Spearman derived from the 10 != test sets derived from the Kfold
    (averageTrain, averagepTrain), # Average Spearman derived from the 10 != train sets derived from the Kfold
    (score_alldata, p_alldata)] # It will be similar to the average since all data is also being used)
    
    
    if population_size == 1000:
        population_size = 5000
    elif population_size == 5000:
        population_size = 10000
    else:
        population_size += 10000
    i += 1
    if i == 100: # Save results from 100 to 100 iterations
        i = 0
        var = results
        results_train = pd.DataFrame.from_dict(var, orient='index')
        results_train.to_csv(path+r'\filename.csv', index=False) # Save file