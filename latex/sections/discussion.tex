\section*{Discussion}


	When we have large food webs, we need to aggregate them to better understand them, especially if we want to simulate them dynamically. The best way of comparing different aggregation methods would be to check which one is the one that maintains the dynamics the best. However, we did not have enough time to see what is the best one in that sense. This is why we decided to use structure as a proxy for dynamics. We used centrality indices as a proxy for network structure. Centrality indices tell us about many things of nodes. This is why we used them, as a proxy for structure. So we can actually say that this is a study where we check whether large food webs can be aggregated into smaller ones by maintaining the properties of the nodes.

	Different centrality indices have different predictive power to find keystone species. The most reliable indices (e.g. keystonenness index, see \citep{Libralato2006}) are the ones based on a dynamical model of the network. However, building a good dynamical model is really time consuming and might not be feasible for multiple networks.	This is why often researchers use topological indices, such as the ones studied in this work. To check the reliability of topological indices, \citet{Gouveia2020} tested how much they aggreed with the dynamical index keystoneness (KS).	They found that the most reliable topological index was the weighted degree (wDC). It could predict the most important species 70.06\% of the times. It was followed by the 5-step weighted topological importance (WI5).	A combination of wDC and WI5 increased this percentage to 78.42\%. It seems like a good aggregation should then maintain these two indices. Another index that a good aggregation method should maintain is the trophic level. This can be calculated, for example, as the trophic position in this study. Trophic level is associated with many biological properties of species. For example, the direction by which a perturbation propagates to \citep{Curtsdotter2011}.	Or the risk of extinction of a species \citep{Binzer2011}. Or the success of a species to colonise a new environment \citep{Holt2010}. (Add: \citet{Gouveia2020} didn't take STO and wSTO into consideration, as well as the trophic level.)

	By looking at the results in Figure \ref{fig:kendall_results}, we see clear differences between algorithms. Density modularity is the worst aggregation algorithm. This is probably because it produces communities based on other factors (such as phylogeny, body mass, and habitat structure, see \citet{Rezende2009}) rather than trophic links between modules. Predator modularity, prey modularity and the group model also performed poorly, considering they performed worse than REGE and Jaccard across all centrality indices (except for predator modularity that performed the best with regards to trophic position). Jaccard and REGE seem to be the best aggregation methods.	They both maintained well the rankings of all centrality indices.	Jaccard maintained the most number of centrality indices slightly better than REGE.	However, REGE maintained better the pattern of the two most important indices according to \citet{Gouveia2020}, wDC and WI5. In light of these findings, we suggest to use either the Jaccard index or the REGE index for aggregation in keystone species research. The clusters produced by these two similarity indices can be useful to answer different questions. The Jaccard index maintains all the information about a certain network, but it doesn't allow you to compare different food webs \citep{Luczkovich2003}. The REGE index produces a controlled loss of information but it allows to compare between different food webs \citep{Luczkovich2003}.

	Future work should be focused on developing new algorithms for the aggregation of species in food webs. It should follow five steps:
		\begin{enumerate}
			\item Developing a singular index that tells us the goodness of the aggregation.
			\item Deciding what is the best method to assign the interaction strength of the link between the clusters.
			\item Deciding what is the best percentage of realised links between clusters to decide that there is a link between them.
			\item Deciding what are the indices we should use. Are more indices better or worse? Should we combine all indices? Or should we develop different aggregation methods according to the type of pattern we want to preserve (e.g., redundancy, capacity of spreading an effect)?
		\end{enumerate}
	This research, however, is complicated by our lack of understanding of keystone species. It is not clear what the best centrality indices are. Also it seems like trophic links might not be the only interactions important in determining keystone species.	\citet{Donohue2017} found that secondary extinctions were better modelled when they took into consideration competition.	This might mean that a multilayer approach (see \citet{Pilosof2017}) could be a better way of finding keystone species.	So we would argue that developing algorithms of aggregation should go hand in hand with this type of research. Another possible direction is checking how different data aggregations influence dynamical indices instead of topological indices. For example, the keystonenness index (KS) \citep{Libralato2006} or eigenvector centrality \citep{Allesina2009}.

	Another possible future direction is using new algorithms for clustering.	These could be hierarchical clustering with different types of similarity indices (e.g., automorphic equivalence \citep{Wasserman1994}, Katz similarity \citep{Newman2018}). Or they could be new algorithms of directed network clusterings (see the comprehensive review of \citet{Malliaros2013}). For example, \citet{Zhou2005,Huang2006,Wang2008,Kim2010} and \citet{Zhan2011} have never been cited by the ecological literature (according to Google Scholar) and their application to food webs might reveal to be useful.

	An interesting way of using aggregation, however, could be to find keystone species.
	It has been suggested by \citep{Bond1994} that keystone species might be the species that cannot be aggregated.
	This makes us wonder whether it would be possible to use aggregation to find keystone species in topological networks.
	And whether they would work better than centrality indices.

	We always need to remind ourselves that aggregated food webs bias our analyses.
	Ultimately, the question should be also whether we should aggregate our data.
	Aggregating entities in ecology has been found to always bias models and produce worse results.
	For example, \citet{Woodward2010b} showed how using the Allometric Diet Breadth Model (ADBM) \citep{Petchey2008} to predict the links between nodes predicted 52\% of the links when building a species-based food web.
	This number increased to 83\% when considering size-based classes.
	Another example is from \citet{Petchey2002}.
	When species were put into functional groups, the community showed high functional redundancy.
	When single species were considered singularly, the community showed low functional redundancy.
	We might argue that the best way of working with food webs is not aggregating. However, the problem is that it is never possible to do this.
	This is because we have a limited resolution power when we sample and the data that have been collected in the past and are avaible for our analyses have low resolution, at least in some part of the network.
	This could be solved by the way we sample.
	Ideally, we should develop technologies and methods that allow us to sample a food web in its highest resolution possible.
	However, this might be really expensive and ecology might not have enough money to finance this type of research.
	This should be especially directed toward charachtering the lowest trophic levels of the food web, for example plankton in aquatic food webs.
	Aggregation research should also inform sampling methods and how to deal with missing data (see \citet{Patonai2017}).

	It is also important to remember that it seems like species who are unique trophospecies seem to be really important for secondary extinction. In particular, \citep{Petchey2008a} found that they are particularly vulnerable to secondary extinctions when trying to model their dynamic food web. The fact that the concept of trophospecies not only is important to understand which ones are the most vulnerable species in the system, but it seems also important in something related to how different species are related to each other. If, as someone said but I don't remember who, keystone species are the ones that are unique in their trophospecies, it means that secondary extinctions happen only if you hit the network close or on keystone species. This is because keystone species, if my interpretation is correct, are not only the most important, but also the most vulnerable. This might be due to the fact that they have such low abundance as well.
	Keystone species might be also the ones that we cannot aggregate in a food web \citep{Bond1994}. So, at this point, maybe data aggregation would reveal keystone species. The fact that the aggregation of according to Jaccard not only can reveal the keystone species, but also would maintain the relative importance of the nodes, seems like a great way of aggregating data. Wait: I need to check whether the species that are unique in their cluster also have high centrality indices.
